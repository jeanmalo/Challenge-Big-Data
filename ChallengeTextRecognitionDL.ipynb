{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge TextRecognition Apprentissage Profond\n",
    "Auteur : JMLD - AD - CN\n",
    "Date : 30/01/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import zipfile\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cible    object\n",
       "Mot1     object\n",
       "Mot2     object\n",
       "Mot3     object\n",
       "Mot4     object\n",
       "Mot5     object\n",
       "Mot6     object\n",
       "Mot7     object\n",
       "Mot8     object\n",
       "Mot9     object\n",
       "Mot10    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"../IntroductionDL/\"\n",
    "trainf = \"calldesk-intents.txt\"\n",
    "dataf = datapath + trainf\n",
    "train = pd.read_csv(dataf,names=['Cible','Mot1','Mot2','Mot3','Mot4','Mot5','Mot6','Mot7','Mot8','Mot9','Mot10'],dtype=str,delim_whitespace=True)\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cible</th>\n",
       "      <th>Mot1</th>\n",
       "      <th>Mot2</th>\n",
       "      <th>Mot3</th>\n",
       "      <th>Mot4</th>\n",
       "      <th>Mot5</th>\n",
       "      <th>Mot6</th>\n",
       "      <th>Mot7</th>\n",
       "      <th>Mot8</th>\n",
       "      <th>Mot9</th>\n",
       "      <th>Mot10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confirm</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confirm</td>\n",
       "      <td>absolument</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Confirm</td>\n",
       "      <td>ah</td>\n",
       "      <td>oui</td>\n",
       "      <td>ça</td>\n",
       "      <td>c</td>\n",
       "      <td>est</td>\n",
       "      <td>bien</td>\n",
       "      <td>ça</td>\n",
       "      <td>comme</td>\n",
       "      <td>ça</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Confirm</td>\n",
       "      <td>aller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Confirm</td>\n",
       "      <td>allez</td>\n",
       "      <td>c</td>\n",
       "      <td>est</td>\n",
       "      <td>bon</td>\n",
       "      <td>pour</td>\n",
       "      <td>$TIME$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cible        Mot1 Mot2 Mot3 Mot4  Mot5    Mot6 Mot7   Mot8 Mot9 Mot10\n",
       "0  Confirm          OK  NaN  NaN  NaN   NaN     NaN  NaN    NaN  NaN   NaN\n",
       "1  Confirm  absolument  NaN  NaN  NaN   NaN     NaN  NaN    NaN  NaN   NaN\n",
       "2  Confirm          ah  oui   ça    c   est    bien   ça  comme   ça   NaN\n",
       "3  Confirm       aller  NaN  NaN  NaN   NaN     NaN  NaN    NaN  NaN   NaN\n",
       "4  Confirm       allez    c  est  bon  pour  $TIME$  NaN    NaN  NaN   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le.fit(train['Cible'])\n",
    "list(le.classes_)\n",
    "train['Cible'] = le.transform(train['Cible'])\n",
    "\n",
    "#pd.get_dummies\n",
    "train_Mot1 = pd.get_dummies(train['Mot1'])\n",
    "train_Mot2 = pd.get_dummies(train['Mot2'])\n",
    "train_Mot3 = pd.get_dummies(train['Mot3'])\n",
    "train_Mot4 = pd.get_dummies(train['Mot4'])\n",
    "train_Mot5 = pd.get_dummies(train['Mot5'])\n",
    "train_Mot6 = pd.get_dummies(train['Mot6'])\n",
    "train_Mot7 = pd.get_dummies(train['Mot7'])\n",
    "train_Mot8 = pd.get_dummies(train['Mot8'])\n",
    "train_Mot9 = pd.get_dummies(train['Mot9'])\n",
    "train_Mot10 = pd.get_dummies(train['Mot10'])\n",
    "train_new = pd.concat([train,train_Mot1,train_Mot2,train_Mot3,train_Mot4,train_Mot5,train_Mot6,train_Mot7,train_Mot8,train_Mot9,train_Mot10],axis=1)\n",
    "\n",
    "#Suppression des doublons\n",
    "\n",
    "del train_new['Mot1']\n",
    "del train_new['Mot2']\n",
    "del train_new['Mot3']\n",
    "del train_new['Mot4']\n",
    "del train_new['Mot5']\n",
    "del train_new['Mot6']\n",
    "del train_new['Mot7']\n",
    "del train_new['Mot8']\n",
    "del train_new['Mot9']\n",
    "del train_new['Mot10']\n",
    "\n",
    "\n",
    "#del train_new['Finess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cible</th>\n",
       "      <th>$TIME$</th>\n",
       "      <th>OK</th>\n",
       "      <th>a-t-il</th>\n",
       "      <th>absolument</th>\n",
       "      <th>accessible</th>\n",
       "      <th>ah</th>\n",
       "      <th>ajouter</th>\n",
       "      <th>aller</th>\n",
       "      <th>allez</th>\n",
       "      <th>...</th>\n",
       "      <th>prochain</th>\n",
       "      <th>prochains</th>\n",
       "      <th>que</th>\n",
       "      <th>rendez-vous</th>\n",
       "      <th>revoir</th>\n",
       "      <th>s</th>\n",
       "      <th>te</th>\n",
       "      <th>une</th>\n",
       "      <th>voir</th>\n",
       "      <th>vous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 891 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cible  $TIME$  OK  a-t-il  absolument  accessible  ah  ajouter  aller  \\\n",
       "0      0       0   1       0           0           0   0        0      0   \n",
       "1      0       0   0       0           1           0   0        0      0   \n",
       "2      0       0   0       0           0           0   1        0      0   \n",
       "3      0       0   0       0           0           0   0        0      1   \n",
       "4      0       0   0       0           0           0   0        0      0   \n",
       "\n",
       "   allez  ...   prochain  prochains  que  rendez-vous  revoir  s  te  une  \\\n",
       "0      0  ...          0          0    0            0       0  0   0    0   \n",
       "1      0  ...          0          0    0            0       0  0   0    0   \n",
       "2      0  ...          0          0    0            0       0  0   0    0   \n",
       "3      0  ...          0          0    0            0       0  0   0    0   \n",
       "4      1  ...          0          0    0            0       0  0   0    0   \n",
       "\n",
       "   voir  vous  \n",
       "0     0     0  \n",
       "1     0     0  \n",
       "2     0     0  \n",
       "3     0     0  \n",
       "4     0     0  \n",
       "\n",
       "[5 rows x 891 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ah       1\n",
       "oui      1\n",
       "ça       1\n",
       "c        1\n",
       "est      1\n",
       "bien     1\n",
       "ça       1\n",
       "comme    1\n",
       "ça       1\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.iloc[2][train_new.iloc[2]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def kfolds(k,N,seed=None):\n",
    "    random.seed(seed)\n",
    "    out = [ list() for _ in range(k) ]\n",
    "    for n in range(N): out[random.randrange(k)].append(n)\n",
    "    return(out)\n",
    "\n",
    "K = kfolds(seed=42,k=5,N=len(train_new))\n",
    "fold1 = sum(K[1:5],[]) # K[1] + K[2] + K[3] + K[4]\n",
    "#fold1 = K[1]+K[2]+K[3]+K[4]\n",
    "xtrain = train_new.iloc[fold1][train_new.keys()[1:]]\n",
    "ytrain = train_new.iloc[fold1][train_new.keys()[0]]\n",
    "\n",
    "xtest = train_new.iloc[K[0]][train_new.keys()[1:]]\n",
    "ytest = train_new.iloc[K[0]][train_new.keys()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitclf = clf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87341772151898733"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitclf.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitgbc = gbc.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83544303797468356"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitgbc.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of nltk to improve the data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nous', 'somm', 'le', 'mond']\n"
     ]
    }
   ],
   "source": [
    "import snowballstemmer\n",
    "\n",
    "stemmer = snowballstemmer.stemmer('french');\n",
    "print(stemmer.stemWords(\"Nous sommes le monde\".split()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train =  pd.DataFrame({('a'): {('A'): 'vache', ('B'): 'vache'},\n",
    "   ....:               ('b'): {('A'): 'cochon', ('C'): 'uluberlu'},\n",
    "   ....:               ('c'): {('A'): 'cochonne', ('D'): 'bouzouk'},\n",
    "   ....:               ('a'): {('B'): 'vive le vent dhiver', ('C'): 'pirate'},\n",
    "   ....:               ('b'): {('B'): 'loup', ('C'): 'aretes'}})\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cochonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>vive le vent dhiver</td>\n",
       "      <td>loup</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>pirate</td>\n",
       "      <td>aretes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bouzouk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     a       b         c\n",
       "A                  NaN     NaN  cochonne\n",
       "B  vive le vent dhiver    loup       NaN\n",
       "C               pirate  aretes       NaN\n",
       "D                  NaN     NaN   bouzouk"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    object\n",
       "b    object\n",
       "c    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemming(x) :\n",
    "    return stemmer.stemWords(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pirat']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(train['a']['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-46efe8e7ca00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-93cc59c8dac3>\u001b[0m in \u001b[0;36mstemming\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "train['a'] = train['a'].apply(stemming(train['a']['A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize('vache aze sdfsdf zerzer poulet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vache', 'aze', 'sdfsdf', 'zerzer', 'poulet']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vach', 'az', 'sdfsdf', 'zerz', 'poulet']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cible</th>\n",
       "      <th>Mot1</th>\n",
       "      <th>Mot2</th>\n",
       "      <th>Mot3</th>\n",
       "      <th>Mot4</th>\n",
       "      <th>Mot5</th>\n",
       "      <th>Mot6</th>\n",
       "      <th>Mot7</th>\n",
       "      <th>Mot8</th>\n",
       "      <th>Mot9</th>\n",
       "      <th>Mot10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>absolument</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ah</td>\n",
       "      <td>oui</td>\n",
       "      <td>ça</td>\n",
       "      <td>c</td>\n",
       "      <td>est</td>\n",
       "      <td>bien</td>\n",
       "      <td>ça</td>\n",
       "      <td>comme</td>\n",
       "      <td>ça</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>aller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>allez</td>\n",
       "      <td>c</td>\n",
       "      <td>est</td>\n",
       "      <td>bon</td>\n",
       "      <td>pour</td>\n",
       "      <td>$TIME$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cible        Mot1 Mot2 Mot3 Mot4  Mot5    Mot6 Mot7   Mot8 Mot9 Mot10\n",
       "0      0          OK  NaN  NaN  NaN   NaN     NaN  NaN    NaN  NaN   NaN\n",
       "1      0  absolument  NaN  NaN  NaN   NaN     NaN  NaN    NaN  NaN   NaN\n",
       "2      0          ah  oui   ça    c   est    bien   ça  comme   ça   NaN\n",
       "3      0       aller  NaN  NaN  NaN   NaN     NaN  NaN    NaN  NaN   NaN\n",
       "4      0       allez    c  est  bon  pour  $TIME$  NaN    NaN  NaN   NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(train['Mot1'])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['OK'],\n",
       " ['absolument'],\n",
       " ['ah'],\n",
       " ['aller'],\n",
       " ['allez'],\n",
       " ['alors'],\n",
       " ['bah'],\n",
       " ['ben'],\n",
       " ['bon'],\n",
       " ['bon'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['entendu'],\n",
       " ['il'],\n",
       " ['iphone'],\n",
       " ['j'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['moi'],\n",
       " ['moi'],\n",
       " ['non'],\n",
       " ['ok'],\n",
       " ['on'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['parfait'],\n",
       " ['si'],\n",
       " ['très'],\n",
       " ['voilà'],\n",
       " ['voilà'],\n",
       " ['voilà'],\n",
       " ['à'],\n",
       " ['ça'],\n",
       " ['ça'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['alors'],\n",
       " ['avez-vous'],\n",
       " ['bon'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['ca'],\n",
       " ['ca'],\n",
       " ['ca'],\n",
       " ['ca'],\n",
       " ['ce'],\n",
       " ['en'],\n",
       " ['encore'],\n",
       " ['encore'],\n",
       " ['esprit'],\n",
       " ['est'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['impossible'],\n",
       " ['j'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['le'],\n",
       " ['mais'],\n",
       " ['ni'],\n",
       " ['nom'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['non'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['pas'],\n",
       " ['pas'],\n",
       " ['peux-tu'],\n",
       " ['plait'],\n",
       " ['plait'],\n",
       " ['plus'],\n",
       " ['plus'],\n",
       " ['plus'],\n",
       " ['tu'],\n",
       " ['un'],\n",
       " ['vous'],\n",
       " ['vous'],\n",
       " ['vous'],\n",
       " ['ça'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['ajouter'],\n",
       " ['aller'],\n",
       " ['alors'],\n",
       " ['alors'],\n",
       " ['avoir'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['dis-moi'],\n",
       " ['dis-moi'],\n",
       " ['donnez'],\n",
       " ['donnez'],\n",
       " ['en'],\n",
       " ['envoie'],\n",
       " ['essaye'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['hi'],\n",
       " ['il'],\n",
       " ['il'],\n",
       " ['il'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['jouons'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['le'],\n",
       " ['madame'],\n",
       " ['mon'],\n",
       " ['non'],\n",
       " ['nouveau'],\n",
       " ['nouveau'],\n",
       " ['nouveau'],\n",
       " ['okay'],\n",
       " ['ouais'],\n",
       " ['ouais'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['oui'],\n",
       " ['prendre'],\n",
       " ['prendre'],\n",
       " ['prendre'],\n",
       " ['prendre'],\n",
       " ['prendre'],\n",
       " ['prends'],\n",
       " ['qu'],\n",
       " ['rendez-vous'],\n",
       " ['rendez-vous'],\n",
       " ['rendez-vous'],\n",
       " ['rendez-vous'],\n",
       " ['répondre'],\n",
       " ['réservez'],\n",
       " ['réservez'],\n",
       " ['tu'],\n",
       " ['tu'],\n",
       " ['tu'],\n",
       " ['vous'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['$', 'TIME', '$'],\n",
       " ['a-t-il'],\n",
       " ['accessible'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['ah'],\n",
       " ['annonce'],\n",
       " ['attendez'],\n",
       " ['attendez'],\n",
       " ['auriez-vous'],\n",
       " ['autant'],\n",
       " ['avec'],\n",
       " ['avez-vous'],\n",
       " ['avez-vous'],\n",
       " ['avez-vous'],\n",
       " ['avez-vous'],\n",
       " ['avez-vous'],\n",
       " ['besoin'],\n",
       " ['bon'],\n",
       " ['bonjour'],\n",
       " ['bonjour'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['c'],\n",
       " ['ce'],\n",
       " ['dans'],\n",
       " ['de'],\n",
       " ['demander'],\n",
       " ['dis-moi'],\n",
       " ['donne-moi'],\n",
       " ['du'],\n",
       " ['encore'],\n",
       " ['enfaite'],\n",
       " ['enfaite'],\n",
       " ['envoyez-moi'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['est-ce'],\n",
       " ['et'],\n",
       " ['finalement'],\n",
       " ['il'],\n",
       " ['il'],\n",
       " ['il'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['j'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['je'],\n",
       " ['la'],\n",
       " ['mais'],\n",
       " ['mais'],\n",
       " ['mais'],\n",
       " ['moi'],\n",
       " ['moi']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenstext = word_tokenize(train['Mot1'][1])\n",
    "tokenstext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-7a66372ea01c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokenstext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mot1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokenstext = []\n",
    "for i in range (0,l):\n",
    "    print(i)\n",
    "    tokenstext.append(word_tokenize(train['Mot1'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolu']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokenstext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-71bf48960f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mot1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Mot1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2292\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:66116)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-71bf48960f11>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mot1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Mot1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "train['Mot1'] = train[\"Mot1\"].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
